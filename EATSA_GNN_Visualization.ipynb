{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiGVF8KI9Ouu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import Module, Linear, Dropout, ModuleList\n",
        "\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset_root = \"./tmp\"\n",
        "dataset_name = \"MUTAG\"\n",
        "\n",
        "# Loading MUTAG dataset\n",
        "dataset = TUDataset(root=dataset_root, name=dataset_name)\n",
        "\n",
        "# get data the first graph of the MUTAG\n",
        "data = dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "dataset = TUDataset(root=\"./data\", name=\"MUTAG\")\n",
        "\n",
        "# Print dataset properties for more understanding of the graph and his properties\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")\n",
        "print(f\"Number of node features: {dataset.num_node_features}\")\n",
        "\n",
        "# Print properties of the first graph in the dataset\n",
        "data = dataset[0]\n",
        "print(\"\\nFirst graph properties:\")\n",
        "print(f\"Number of nodes: {data.num_nodes}\")\n",
        "print(f\"Number of edges: {data.num_edges}\")\n",
        "print(f\"Average node degree: {data.num_edges / data.num_nodes:.2f}\")\n",
        "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
        "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
        "print(f\"Is undirected: {data.is_undirected()}\")\n",
        "\n",
        "# we get and combine  properties for all graphs in the dataset\n",
        "total_nodes = sum([graph.num_nodes for graph in dataset])\n",
        "total_edges = sum([graph.num_edges for graph in dataset])\n",
        "avg_nodes = total_nodes / len(dataset)\n",
        "avg_edges = total_edges / len(dataset)\n",
        "\n",
        "print(\"\\nOverall dataset properties:\")\n",
        "print(f\"Total number of nodes: {total_nodes}\")\n",
        "print(f\"Total number of edges: {total_edges}\")\n",
        "print(f\"Average number of nodes per graph: {avg_nodes:.2f}\")\n",
        "print(f\"Average number of edges per graph: {avg_edges:.2f}\")"
      ],
      "metadata": {
        "id": "PWWDxVPn-HVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool # Correct import statement\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "# ... rest of the code remains the same\n",
        "\n",
        "\n",
        "dataset = TUDataset(root=\"./data\", name=\"MUTAG\")\n",
        "\n",
        "# Verify dataset length\n",
        "print(f\"Total number of graphs in the dataset: {len(dataset)}\")\n",
        "\n",
        "# Print dataset properties get more undertanding\n",
        "total_nodes = sum([graph.num_nodes for graph in dataset])\n",
        "total_edges = sum([graph.num_edges for graph in dataset])\n",
        "avg_nodes = total_nodes / len(dataset)\n",
        "avg_edges = total_edges / len(dataset)\n",
        "print(f\"Number of classes: {dataset.num_classes}\")\n",
        "print(f\"Number of node features: {dataset.num_node_features}\")\n",
        "print(f\"Total number of nodes: {total_nodes}\")\n",
        "print(f\"Total number of edges: {total_edges}\")\n",
        "print(f\"Average number of nodes per graph: {avg_nodes:.2f}\")\n",
        "print(f\"Average number of edges per graph: {avg_edges:.2f}\")\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "num_train = int(0.8 * len(dataset))\n",
        "num_val = int(0.1 * len(dataset))\n",
        "num_test = len(dataset) - num_train - num_val\n",
        "\n",
        "train_dataset = dataset[:num_train]\n",
        "val_dataset = dataset[num_train:num_train + num_val]\n",
        "test_dataset = dataset[num_train + num_val:]\n",
        "\n",
        "# Verifications of  sizes of each split\n",
        "print(f\"Number of training graphs: {len(train_dataset)}\")\n",
        "print(f\"Number of validation graphs: {len(val_dataset)}\")\n",
        "print(f\"Number of test graphs: {len(test_dataset)}\")\n",
        "\n",
        "# DataLoaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class ETSA(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_gcn_layers=1, attention_type='sigmoid', dropout_rate=0.5):\n",
        "        super(ETSA, self).__init__()\n",
        "        self.embedding_layer = nn.Linear(num_features, hidden_dim)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "        self.gcn_layers = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(self.num_gcn_layers)])\n",
        "\n",
        "        if attention_type == 'sigmoid':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, 1)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, 1)\n",
        "        elif attention_type == 'softmax':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.final_gcn_layer = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.embedding_layer(x)\n",
        "        for i in range(self.num_gcn_layers):\n",
        "            x = F.relu(self.gcn_layers[i](x, edge_index))\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        stage_1_attention_weights = torch.sigmoid(self.stage_1_attention(x))\n",
        "        stage_1_representation = torch.sum(x * stage_1_attention_weights, dim=0)\n",
        "\n",
        "        stage_2_attention_weights = torch.sigmoid(self.stage_2_attention(x))\n",
        "        stage_2_representation = torch.sum(x * stage_2_attention_weights, dim=0)\n",
        "\n",
        "        combined_representation = stage_1_representation + stage_2_representation\n",
        "\n",
        "        x = self.final_gcn_layer(x, edge_index)\n",
        "\n",
        "        # Aggregate node embeddings to graph-level embeddings\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        return x, combined_representation\n",
        "\n",
        "# Model, criterion, optimizer\n",
        "model = ETSA(num_features=dataset.num_node_features, hidden_dim=64, num_classes=dataset.num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Using CrossEntropyLoss for classification tasks\n",
        "optimizer = Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def fit():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    y_true, y_pred = [], []\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out, _ = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "        # Ensure the output and target dimensions match\n",
        "        out = out.squeeze()\n",
        "        target = data.y\n",
        "\n",
        "        assert out.size(0) == target.size(0), f\"Output size: {out.size()}, Target size: {target.size()}\"\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y_true.append(target.detach().cpu())\n",
        "        y_pred.append(out.max(1)[1].detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
        "    accuracy, precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
        "    return total_loss / len(train_loader.dataset), accuracy\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred, y_scores = [], [], []\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out, _ = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "        out = out.squeeze()\n",
        "        target = data.y\n",
        "\n",
        "        y_true.append(target.cpu())\n",
        "        y_pred.append(out.max(1)[1].cpu())\n",
        "        y_scores.append(F.softmax(out, dim=1)[:, 1].cpu().numpy())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).detach().numpy()\n",
        "    y_pred = torch.cat(y_pred, dim=0).detach().numpy()\n",
        "    y_scores = np.concatenate(y_scores, axis=0)\n",
        "\n",
        "    accuracy, precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
        "    auc = compute_auc(y_true, y_scores)\n",
        "    return accuracy, precision, recall, f1, auc, y_scores\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lists to store metrics for plotting for the research paper , if more metrics are needed we will add them in this part.\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "val_f1s = []\n",
        "val_aucs = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(201):\n",
        "    loss, train_accuracy = fit()\n",
        "    val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate(val_loader)[:5]  # Exclude y_scores from evaluation result\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_precisions.append(val_precision)\n",
        "    val_recalls.append(val_recall)\n",
        "    val_f1s.append(val_f1)\n",
        "    val_aucs.append(val_auc)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}% | Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
        "\n",
        "# Final evaluation on the test set\n",
        "test_accuracy, test_precision, test_recall, test_f1, test_auc, test_y_scores = evaluate(test_loader)\n",
        "\n",
        "print(f'Test Acc: {test_accuracy:.2f}% | Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(train_accuracies)), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hm9FkWt7-RgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center>  Review of the EASTA Model\n",
        "Based on the ETSA model according to the feedback from our research reviewer, this explanation goes into detail on the requested topics:\n",
        "\n",
        "## Attention Weights Visualization\n",
        "\n",
        "1. The ETSA model uses the Edge-Aware and Two-Stage Attention mechanisms to focus on certain nodes and edges during the classification. To visualize these attention weights, we can use the following approach:\n",
        "\n",
        "2. Extracting Attention Weights: In the first stage, we calculate attention weights by applying a sigmoid function to the hidden node features. In the second stage, we again calculate attention weights by using another sigmoid function over the node features after the first attention mechanism.\n",
        "\n",
        "\n",
        "3. Displaying Attention Weights: We can visualize the attention weights by highlighting the nodes and edges in a graph. Nodes and edges with higher attention weights can be colored more intensely to indicate their importance in the classification decision. Mathematically,if\\ \\( \\mathbf{h}_i \\) represents the hidden features of node \\( i \\), the attention weight \\( \\alpha_i \\) in the first stage can be computed as:\n",
        "    \\[\n",
        "    \\alpha_i = \\sigma(\\mathbf{w}_1^T \\mathbf{h}_i)\n",
        "    \\]\n",
        "    where \\( \\sigma \\) is the sigmoid function and \\( \\mathbf{w}_1 \\) is a learnable weight vector.\n",
        "\n",
        "    Similarly, in the second stage:\n",
        "    \\[\n",
        "    \\beta_i = \\sigma(\\mathbf{w}_2^T \\mathbf{h}_i)\n",
        "    \\]\n",
        "    where \\( \\mathbf{w}_2 \\) is another learnable weight vector.\\\\\n",
        "\n"
      ],
      "metadata": {
        "id": "z1DNnur6-XEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# Define the ETSA model with the correct dimensions\n",
        "class ETSA(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_gcn_layers=1, attention_type='sigmoid', dropout_rate=0.5):\n",
        "        super(ETSA, self).__init__()\n",
        "        self.embedding_layer = nn.Linear(num_features, hidden_dim)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "        self.gcn_layers = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(self.num_gcn_layers)])\n",
        "\n",
        "        if attention_type == 'sigmoid':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, 1)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, 1)\n",
        "        elif attention_type == 'softmax':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.final_gcn_layer = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.embedding_layer(x)\n",
        "        for i in range(self.num_gcn_layers):\n",
        "            x = F.relu(self.gcn_layers[i](x, edge_index))\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        stage_1_attention_weights = torch.sigmoid(self.stage_1_attention(x))\n",
        "        stage_1_representation = torch.sum(x * stage_1_attention_weights, dim=0)\n",
        "\n",
        "        stage_2_attention_weights = torch.sigmoid(self.stage_2_attention(x))\n",
        "        stage_2_representation = torch.sum(x * stage_2_attention_weights, dim=0)\n",
        "\n",
        "        combined_representation = stage_1_representation + stage_2_representation\n",
        "\n",
        "        x = self.final_gcn_layer(x, edge_index)\n",
        "\n",
        "        # Aggregate node embeddings to graph-level embeddings\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        return x, (stage_1_attention_weights, stage_2_attention_weights)\n",
        "\n",
        "# Load the MUTAG dataset\n",
        "dataset = TUDataset(root=\"./data\", name=\"MUTAG\")\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "num_train = int(0.8 * len(dataset))\n",
        "num_val = int(0.1 * len(dataset))\n",
        "num_test = len(dataset) - num_train - num_val\n",
        "\n",
        "train_dataset = dataset[:num_train]\n",
        "val_dataset = dataset[num_train:num_train + num_val]\n",
        "test_dataset = dataset[num_train + num_val:]\n",
        "\n",
        "# DataLoaders for train, validation, and test sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model, criterion, optimizer\n",
        "model = ETSA(num_features=dataset.num_node_features, hidden_dim=64, num_classes=dataset.num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss()  # Using CrossEntropyLoss for classification tasks\n",
        "optimizer = Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    auc = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovr')\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "def fit():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    y_true, y_pred = [], []\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out, _ = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "        # Ensure the output and target dimensions match\n",
        "        out = out.squeeze()  # Remove singleton dimensions\n",
        "        target = data.y  # Target labels\n",
        "\n",
        "        assert out.size(0) == target.size(0), f\"Output size: {out.size()}, Target size: {target.size()}\"\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        y_true.append(target.detach().cpu())\n",
        "        y_pred.append(out.max(1)[1].detach().cpu())  # Get the index of the max log-probability\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim=0).numpy()\n",
        "    accuracy, precision, recall, f1, auc = compute_metrics(y_true, y_pred)\n",
        "    return total_loss / len(train_loader.dataset), accuracy, precision, recall, f1, auc\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out, _ = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "        out = out.squeeze()  # Remove singleton dimensions\n",
        "        target = data.y  # Target labels\n",
        "\n",
        "        y_true.append(target.cpu())\n",
        "        y_pred.append(out.max(1)[1].cpu())  # Get the index of the max log-probability\n",
        "\n",
        "    y_true = torch.cat(y_true, dim=0).detach().numpy()\n",
        "    y_pred = torch.cat(y_pred, dim=0).detach().numpy()\n",
        "\n",
        "    accuracy, precision, recall, f1, auc = compute_metrics(y_true, y_pred)\n",
        "    return accuracy, precision, recall, f1, auc\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(301):\n",
        "    loss, train_accuracy, train_precision, train_recall, train_f1, train_auc = fit()\n",
        "    val_accuracy, val_precision, val_recall, val_f1, val_auc = evaluate(val_loader)\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}% | Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f} | Val AUC: {val_auc:.4f}')\n",
        "\n",
        "# Final evaluation on the test set\n",
        "test_accuracy, test_precision, test_recall, test_f1, test_auc = evaluate(test_loader)\n",
        "print(f'Test Acc: {test_accuracy:.2f}% | Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f} | Test F1: {test_f1:.4f} | Test AUC: {test_auc:.4f}')\n",
        "\n",
        "# Extracting attention weights for visualization\n",
        "data = next(iter(test_loader)).to(device)\n",
        "_, (stage_1_attention_weights, stage_2_attention_weights) = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "# Visualizing the attention weights using NetworkX\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# Visualizing stage 1 attention weights\n",
        "node_attention_1 = stage_1_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_1 = node_attention_1 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_1, node_color='skyblue', edge_color='gray')\n",
        "plt.title(\"Stage 1 Attention Weights\")\n",
        "plt.show()\n",
        "\n",
        "# Visualizing stage 2 attention weights\n",
        "node_attention_2 = stage_2_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_2 = node_attention_2 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_2, node_color='lightgreen', edge_color='gray')\n",
        "plt.title(\"Stage 2 Attention Weights\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "sMIUf8A0-YB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "\n",
        "class ETSA(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5):\n",
        "        super(ETSA, self).__init__()\n",
        "        self.embedding_layer = nn.Linear(num_features, hidden_dim)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "        self.gcn_layers = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(self.num_gcn_layers)])\n",
        "\n",
        "        if attention_type == 'sigmoid':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, 1)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, 1)\n",
        "        elif attention_type == 'softmax':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.final_gcn_layer = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.embedding_layer(x)\n",
        "        for i in range(self.num_gcn_layers):\n",
        "            x = F.relu(self.gcn_layers[i](x, edge_index))\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        stage_1_attention_weights = torch.sigmoid(self.stage_1_attention(x))\n",
        "        stage_1_representation = torch.sum(x * stage_1_attention_weights, dim=0)\n",
        "\n",
        "        stage_2_attention_weights = torch.sigmoid(self.stage_2_attention(x))\n",
        "        stage_2_representation = torch.sum(x * stage_2_attention_weights, dim=0)\n",
        "\n",
        "        combined_representation = stage_1_representation + stage_2_representation\n",
        "\n",
        "        x = self.final_gcn_layer(x, edge_index)\n",
        "\n",
        "        return x, (stage_1_attention_weights, stage_2_attention_weights)\n",
        "\n",
        "# Load the Cora dataset for two satges visualizations , ask you ask for that we should use the CORA data\n",
        "cora_dataset = Planetoid(root='data/Cora', name='Cora')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model, criterion, optimizer\n",
        "model = ETSA(num_features=cora_dataset.num_node_features, hidden_dim=64, num_classes=cora_dataset.num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Early stopping\n",
        "best_val_accuracy = 0.0\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Use DataLoader for mini-batch training\n",
        "    for data in DataLoader(cora_dataset, batch_size=1, shuffle=True):\n",
        "        data = data.to(device)\n",
        "        out, _ = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_mask = cora_dataset[0].val_mask\n",
        "        val_logits, _ = model(cora_dataset[0].x.to(device), cora_dataset[0].edge_index.to(device))\n",
        "        val_pred_labels = val_logits.argmax(dim=1)[val_mask]\n",
        "        val_accuracy = accuracy_score(cora_dataset[0].y[val_mask].cpu(), val_pred_labels.cpu())\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Final evaluation on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_mask = cora_dataset[0].test_mask\n",
        "    test_logits, (stage_1_attention_weights, stage_2_attention_weights) = model(cora_dataset[0].x.to(device), cora_dataset[0].edge_index.to(device))\n",
        "    test_pred_labels = test_logits.argmax(dim=1)[test_mask]\n",
        "    test_accuracy = accuracy_score(cora_dataset[0].y[test_mask].cpu(), test_pred_labels.cpu())\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Visualizing the attention weights using NetworkX\n",
        "data = cora_dataset[0]\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# Visualizing stage 1 attention weights\n",
        "node_attention_1 = stage_1_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_1 = node_attention_1 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_1, node_color='skyblue', edge_color='gray')\n",
        "plt.title(\"Stage 1 Attention Weights\")\n",
        "plt.show()\n",
        "\n",
        "# Visualizing stage 2 attention weights\n",
        "node_attention_2 = stage_2_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_2 = node_attention_2 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_2, node_color='lightgreen', edge_color='gray')\n",
        "plt.title(\"Stage 2 Attention Weights\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b1ePnLmV-jUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "\n",
        "# Define the ETSA model\n",
        "class ETSA(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5):\n",
        "        super(ETSA, self).__init__()\n",
        "        self.embedding_layer = nn.Linear(num_features, hidden_dim)\n",
        "        self.num_gcn_layers = num_gcn_layers\n",
        "        self.gcn_layers = nn.ModuleList([GCNConv(hidden_dim, hidden_dim) for _ in range(self.num_gcn_layers)])\n",
        "\n",
        "        if attention_type == 'sigmoid':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, 1)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, 1)\n",
        "        elif attention_type == 'softmax':\n",
        "            self.stage_1_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.stage_2_attention = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.final_gcn_layer = GCNConv(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.embedding_layer(x)\n",
        "        for i in range(self.num_gcn_layers):\n",
        "            x = F.relu(self.gcn_layers[i](x, edge_index))\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        stage_1_attention_weights = torch.sigmoid(self.stage_1_attention(x))\n",
        "        stage_1_representation = torch.sum(x * stage_1_attention_weights, dim=0)\n",
        "\n",
        "        stage_2_attention_weights = torch.sigmoid(self.stage_2_attention(x))\n",
        "        stage_2_representation = torch.sum(x * stage_2_attention_weights, dim=0)\n",
        "\n",
        "        combined_representation = stage_1_representation + stage_2_representation\n",
        "\n",
        "        x = self.final_gcn_layer(x, edge_index)\n",
        "\n",
        "        return x, (stage_1_attention_weights, stage_2_attention_weights)\n",
        "\n",
        "# Load the Cora dataset\n",
        "cora_dataset = Planetoid(root='data/Pubmed', name='Pubmed')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Model, criterion, optimizer\n",
        "model = ETSA(num_features=cora_dataset.num_node_features, hidden_dim=64, num_classes=cora_dataset.num_classes, num_gcn_layers=2, attention_type='sigmoid', dropout_rate=0.5).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# Early stopping\n",
        "best_val_accuracy = 0.0\n",
        "patience = 10\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Use DataLoader for mini-batch training\n",
        "    for data in DataLoader(cora_dataset, batch_size=1, shuffle=True):\n",
        "        data = data.to(device)\n",
        "        out, _ = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation on validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_mask = cora_dataset[0].val_mask\n",
        "        val_logits, _ = model(cora_dataset[0].x.to(device), cora_dataset[0].edge_index.to(device))\n",
        "        val_pred_labels = val_logits.argmax(dim=1)[val_mask]\n",
        "        val_accuracy = accuracy_score(cora_dataset[0].y[val_mask].cpu(), val_pred_labels.cpu())\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(val_accuracy)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Final evaluation on the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_mask = cora_dataset[0].test_mask\n",
        "    test_logits, (stage_1_attention_weights, stage_2_attention_weights) = model(cora_dataset[0].x.to(device), cora_dataset[0].edge_index.to(device))\n",
        "    test_pred_labels = test_logits.argmax(dim=1)[test_mask]\n",
        "    test_accuracy = accuracy_score(cora_dataset[0].y[test_mask].cpu(), test_pred_labels.cpu())\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Visualizing the attention weights using NetworkX\n",
        "data = cora_dataset[0]\n",
        "G = to_networkx(data, to_undirected=True)\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "# Visualizing stage 1 attention weights\n",
        "node_attention_1 = stage_1_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_1 = node_attention_1 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_1, node_color='skyblue', edge_color='gray')\n",
        "plt.title(\"Stage 1 Attention Weights\")\n",
        "plt.show()\n",
        "\n",
        "# Visualizing stage 2 attention weights\n",
        "node_attention_2 = stage_2_attention_weights.cpu().detach().numpy()\n",
        "node_sizes_2 = node_attention_2 * 1000  # Scale for better visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_size=node_sizes_2, node_color='lightgreen', edge_color='gray')\n",
        "plt.title(\"Stage 2 Attention Weights\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gTdYZKCS-zvv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}